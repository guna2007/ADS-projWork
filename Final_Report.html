<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Diabetes Risk Prediction Report</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 40px;
            line-height: 1.6;
        }

        h1 {
            text-align: center;
            color: #333;
        }

        h2 {
            color: #555;
            border-bottom: 2px solid #ddd;
            padding-bottom: 5px;
            margin-top: 30px;
        }

        h3 {
            color: #666;
        }

        img {
            max-width: 700px;
            display: block;
            margin: 20px auto;
            border: 1px solid #ddd;
            padding: 5px;
        }

        .caption {
            text-align: center;
            font-style: italic;
            color: #666;
            margin-bottom: 20px;
        }

        pre {
            background-color: #f4f4f4;
            padding: 10px;
            border-left: 3px solid #333;
            overflow-x: auto;
        }
    </style>
</head>

<body>

    <p style="text-align: center;">
        <strong>Name:</strong> L. Guna, B.Rakesh, IVS.Akhil<br>
        <strong>Roll No:</strong> CS24B2043,CS24B2039,CS24B2005<br>
        <strong>Course:</strong> Applied Data Science<br>
        <strong>Project Title:</strong> Diabetes Risk Prediction Using Logistic Regression and SVM
    </p>

    <h1>Diabetes Risk Prediction Using Logistic Regression and SVM</h1>

    <h2>Introduction</h2>
    <p>This report presents an analysis and predictive modeling study performed on the Pima Indians Diabetes dataset.
        The goal is to explore the dataset through descriptive analytics and build machine learning models that predict
        diabetes outcome. The study demonstrates the use of two algorithms not covered in classroom instruction:
        Logistic Regression and Support Vector Machine (SVM). The results help highlight factors influencing diabetes
        risk and evaluate model effectiveness.</p>

    <h2>Objectives</h2>
    <p>The main objectives of this work are:</p>
    <ol>
        <li>To perform descriptive analytics on the Pima Indians Diabetes dataset and understand the relationships
            between medical features and diabetes outcome.</li>
        <li>To build and compare two binary classification models (Logistic Regression and SVM) that were not covered in
            classroom discussions.</li>
        <li>To evaluate the models using accuracy, precision, recall, F1-score, confusion matrix, and ROC-AUC, and
            interpret the results.</li>
    </ol>

    <h2>Dataset Description</h2>
    <p>The dataset contains 768 instances and 9 attributes. The target column "Outcome" is binary (0 = non-diabetic, 1 =
        diabetic). The input features include:<br>
        Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, Age.<br>
        The dataset includes medical measurements often associated with metabolic health and diabetes risk.</p>

    <h2>Folder Structure</h2>
    <pre>
diabetes_project/
    main.py
    Problem_Statement.txt
    Final_Report.html
    Final_Report.pdf
    README.txt
    requirements.txt
    data/diabetes.csv
    results/(all plot images)
</pre>

    <h2>Code Workflow Explanation</h2>
    <p>The workflow of the project begins with loading the Pima Indians Diabetes dataset into a pandas DataFrame.
        Initial inspection of the data is carried out using descriptive statistics and structural summaries to
        understand the range and distribution of values in each feature. Various exploratory plots are then generated to
        identify key patterns such as feature distribution, class imbalance, outliers, and correlations between medical
        parameters.</p>

    <p>Before applying machine learning models, all input features are standardized using StandardScaler, since Logistic
        Regression and SVM are sensitive to differences in feature scale. Standardization ensures that all features
        contribute proportionally during model training. After preprocessing, the dataset is split into training and
        test sets using an 80:20 split, allowing accurate evaluation of the model on unseen data.</p>

    <p>Two supervised classification models are developed:</p>

    <p>Logistic Regression – a linear probability-based classifier that learns a set of weights for each feature by
        applying Maximum Likelihood Estimation.</p>

    <p>Support Vector Machine (SVM) – a margin-based classifier that identifies the optimal separating hyperplane to
        distinguish diabetic vs. non-diabetic patients.</p>

    <p>Both models are trained on the training subset and then evaluated on the test subset. For evaluation, standard
        performance metrics such as accuracy, precision, recall, F1-score, and confusion matrix are calculated. ROC-AUC
        curves are plotted to assess the ranking and discrimination ability of each model. In addition to numeric
        metrics, graphical comparisons such as ROC curves, accuracy comparison bars, and confusion matrix heatmaps
        provide better insight into the strengths and limitations of each model.</p>

    <p>This workflow ensures a structured pipeline from raw data exploration to predictive modeling and performance
        assessment, supporting meaningful conclusions about diabetes risk detection.</p>

    <h2>Logistic Regression Explanation</h2>
    <p>Logistic Regression is a statistical learning method used for binary classification problems such as predicting
        whether a patient is diabetic or not. It models the relationship between input features and the probability of
        class membership. The model forms a linear combination of the input features:</p>

    <p>z = w₀ + w₁x₁ + w₂x₂ + ... + wₙxₙ</p>

    <p>Since the raw output z can take any real value, it is transformed into a probability using the logistic sigmoid
        function:</p>

    <p>σ(z) = 1 / (1 + e⁻ᶻ)</p>

    <p>The model parameters w are estimated by minimizing the Binary Cross-Entropy loss, which measures the difference
        between predicted probabilities and actual labels. The optimization uses gradient descent to iteratively update
        weights in the direction that reduces loss. A classification threshold (commonly 0.5) converts the predicted
        probability into class labels.</p>

    <p>One major advantage of logistic regression is interpretability: the coefficients indicate how strongly each
        medical feature influences diabetes risk. For example, in our model, glucose has the highest coefficient,
        meaning an increase in glucose level significantly increases the probability of diabetes. This transparency
        makes Logistic Regression well-suited for healthcare, where explanatory support is crucial.</p>

    <h2>Support Vector Machine Explanation</h2>
    <p>Support Vector Machine (SVM) is a margin-based classification algorithm that aims to find the optimal separating
        boundary between classes. Instead of focusing on all training samples, SVM concentrates on the most critical
        points near the decision boundary, called support vectors. The objective is to maximize the margin — the
        distance between support vectors and the separating hyperplane. A wider margin generally improves generalization
        to unseen data.</p>

    <p>The decision function follows the linear form:</p>

    <p>f(x) = wᵀx + b</p>

    <p>Training an SVM involves solving an optimization problem that minimizes the hinge loss:</p>

    <p>Loss = Σ max(0, 1 − yᵢ(wᵀxᵢ + b))</p>

    <p>while simultaneously minimizing the magnitude of the weight vector ||w||. The balance between correct
        classification and margin control enhances robustness against noisy or overlapping data. In this project, a
        linear kernel is used because it is computationally efficient and matches the linear separability seen in our 2D
        visualization (Glucose vs BMI).</p>

    <p>SVM typically performs well on medium-sized medical datasets where avoiding false negatives is important. In
        predictive diagnosis, a false negative means predicting "healthy" for a diabetic patient — a critical risk. Our
        results show that SVM produced fewer false negatives than Logistic Regression, justifying its practical
        advantage.</p>

    <h2>Performance Evaluation</h2>
    <p>Standard evaluation metrics are computed including accuracy, precision, recall, and F1-score. ROC-AUC curves
        visualize the ranking performance of both models. Confusion matrix heatmaps indicate correct vs incorrect
        classifications. Comparing model results helps determine which model performs better on this dataset.</p>

    <h2>Results Summary</h2>
    <p>After training the models and evaluating them on the test set, the numerical values of accuracy, precision,
        recall, F1-score and AUC are reported from the Python output. These values, along with the confusion matrices
        and ROC curves, are used to compare Logistic Regression and SVM.<br>
        In our run, Logistic Regression achieved an accuracy of 78.57% and SVM achieved an accuracy of 79.22%.</p>

    <h2>Plots and Inferences</h2>

    <h3>Exploratory Data Analysis</h3>

    <img src="results/dist_glucose.png" alt="Glucose Distribution">
    <p class="caption">The glucose histogram shows right-skewed distribution indicating many high-glucose cases which
        strongly relate to diabetes outcome.</p>

    <img src="results/missing_values.png" alt="Missing Values">
    <p class="caption">Missing values chart shows no null entries in the dataset. However, some measurements such as
        blood pressure having a value of zero are likely to be invalid rather than true zeros, and they behave like
        hidden missing values.</p>

    <img src="results/dist_bmi.png" alt="BMI Distribution">
    <p class="caption">BMI distribution shows slight right skew with most values in normal to overweight range.</p>

    <img src="results/dist_age.png" alt="Age Distribution">
    <p class="caption">Age distribution shows younger population majority with decreasing frequency in older age groups.
    </p>

    <img src="results/dist_bp.png" alt="Blood Pressure Distribution">
    <p class="caption">Blood pressure distribution appears roughly normal with some outliers at zero indicating missing
        or invalid measurements.</p>

    <img src="results/pairplot.png" alt="Pairplot">
    <p class="caption">Pairplot shows feature relationships with Glucose and BMI showing clear separation between
        outcome classes.</p>

    <img src="results/outliers_box.png" alt="Outliers Boxplot">
    <p class="caption">Boxplots identify outliers in Glucose, BloodPressure, BMI, and Age with several extreme values
        present.</p>

    <img src="results/scatter_glucose_bmi.png" alt="Glucose vs BMI Scatter">
    <p class="caption">Scatter plot of Glucose vs BMI colored by outcome shows diabetic cases concentrated in higher
        glucose and BMI regions.</p>

    <img src="results/violin_bmi_outcome.png" alt="BMI Violin Plot">
    <p class="caption">Violin plot shows BMI distribution is higher for diabetic patients compared to non-diabetic.</p>

    <img src="results/count_preg.png" alt="Pregnancies Count">
    <p class="caption">Countplot shows pregnancy count relationship with outcome indicating higher pregnancy count may
        correlate with diabetes risk.</p>

    <img src="results/class_count.png" alt="Class Count">
    <p class="caption">Class count plot shows dataset imbalance with majority class being non-diabetic.</p>

    <img src="results/corr_heatmap.png" alt="Correlation Heatmap">
    <p class="caption">The correlation heatmap highlights Glucose and BMI as strongly related to diabetes.</p>

    <h3>Machine Learning Preparation</h3>

    <img src="results/coef_importance.png" alt="Feature Importance">
    <p class="caption">Logistic Regression coefficients show Glucose has highest positive influence on diabetes
        prediction.</p>

    <img src="results/decision_boundary_svm.png" alt="SVM Decision Boundary">
    <p class="caption">SVM decision boundary visualization using Glucose and BMI shows linear separation between
        classes.</p>

    <img src="results/corr_sorted.png" alt="Correlation Sorted">
    <p class="caption">Sorted correlation bar chart shows Glucose has strongest positive correlation with outcome.</p>

    <img src="results/grouped_mean.png" alt="Grouped Mean Features">
    <p class="caption">Mean feature values grouped by class show diabetic patients have higher average values for most
        features.</p>

    <h3>Model Performance Visualization</h3>

    <img src="results/conf_matrix_lr.png" alt="Confusion Matrix Logistic Regression">
    <p class="caption">Confusion matrix for Logistic Regression shows good true negative rate but moderate false
        negative rate.</p>

    <img src="results/conf_matrix_svm.png" alt="Confusion Matrix SVM">
    <p class="caption">Confusion matrix for SVM shows slightly better performance than Logistic Regression with fewer
        false negatives.</p>

    <img src="results/roc_compare.png" alt="ROC Comparison">
    <p class="caption">ROC curves comparison shows both models perform similarly with SVM having slightly higher AUC.
    </p>

    <img src="results/accuracy_compare.png" alt="Accuracy Comparison">
    <p class="caption">Accuracy comparison chart shows SVM achieves marginally higher accuracy than Logistic Regression.
    </p>

    <img src="results/prec_rec_f1_compare.png" alt="Precision Recall F1 Comparison">
    <p class="caption">Precision-Recall-F1 score bar graphs show both models have similar precision but SVM has slightly
        better recall.</p>

    <h2>Conclusion</h2>
    <p>This project explored diabetes prediction using two fundamental machine learning classifiers — Logistic
        Regression and Support Vector Machine. Through exploratory data analysis, features such as glucose level, BMI,
        and age were identified as key indicators for diabetes risk. Logistic Regression provided interpretable model
        coefficients valuable for medical understanding, while SVM produced stronger classification boundaries and
        reduced false negatives.</p>

    <p>Performance evaluation using confusion matrices, ROC-AUC, and precision-recall measures demonstrated that SVM
        achieved slightly higher accuracy and better recall performance. This suggests that margin-based learning is
        more effective for this dataset where patient groups overlap in feature space. However, Logistic Regression
        remains preferable for medical reasoning and model transparency.</p>

    <p>Overall, combining clinically interpretable models with strong predictive models can support early diabetes
        screening and decision-making in healthcare. Future extensions could include hyperparameter tuning, class
        balancing techniques (SMOTE), and experimenting with non-linear kernels or ensemble models to further improve
        detection accuracy.</p>

    <p style="text-align: center; margin-top: 40px;"><strong>End of Report</strong></p>

</body>

</html>